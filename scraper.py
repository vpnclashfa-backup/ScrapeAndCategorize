import asyncio
import aiohttp
import json
import re
import logging
from bs4 import BeautifulSoup
import os
import shutil
from datetime import datetime
import pytz
import base64
from urllib.parse import parse_qs, unquote

# --- Configuration ---
URLS_FILE = 'urls.txt'
KEYWORDS_FILE = 'keywords.json'
OUTPUT_DIR = 'output_configs'
README_FILE = 'README.md'
REQUEST_TIMEOUT = 15
CONCURRENT_REQUESTS = 10
MAX_CONFIG_LENGTH = 1500
MIN_PERCENT25_COUNT = 15

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# --- Protocol Categories ---
PROTOCOL_CATEGORIES = [
    "Vmess", "Vless", "Trojan", "ShadowSocks", "ShadowSocksR",
    "Tuic", "Hysteria2", "WireGuard"
]

# --- Helper function to check for Persian-like text ---
def is_persian_like(text):
    """
    Checks if a string is predominantly Persian by looking for Arabic script characters
    and the absence of Latin characters.
    """
    if not isinstance(text, str) or not text.strip():
        return False
    has_persian_char = False
    has_latin_char = False
    for char in text:
        # Arabic Unicode block (covers Persian, Arabic, Urdu, etc.)
        # Ù‡Ù…Ú†Ù†ÛŒÙ† Ø­Ø±ÙˆÙ ÛŒØ§ÛŒ ÙØ§Ø±Ø³ÛŒ Ùˆ Ú©Ø§Ù ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ù¾ÙˆØ´Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
        if '\u0600' <= char <= '\u06FF' or char in ['\u200C', '\u200D']: # ZWNJ and ZWJ
            has_persian_char = True
        elif 'a' <= char.lower() <= 'z':
            has_latin_char = True
        # Ø§Ú¯Ø± Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø±ÛŒ (Ù…Ø«Ù„Ø§ Ø§Ø¹Ø¯Ø§Ø¯ ÛŒØ§ Ø³ÛŒÙ…Ø¨ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬) Ø¨Ø§Ø´Ù†Ø¯ØŒ ÙØ¹Ù„Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
        # Ùˆ ÙÙ‚Ø· Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ¬ÙˆØ¯ ÙØ§Ø±Ø³ÛŒ Ùˆ Ù†Ø¨ÙˆØ¯ Ù„Ø§ØªÛŒÙ† ØªØµÙ…ÛŒÙ… Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ….

    return has_persian_char and not has_latin_char

# --- Base64 Decoding Helper ---
def decode_base64(data):
    try:
        data = data.replace('_', '/').replace('-', '+')
        missing_padding = len(data) % 4
        if missing_padding:
            data += '=' * (4 - missing_padding)
        return base64.b64decode(data).decode('utf-8')
    except Exception:
        return None

# --- Protocol Name Extraction Helpers ---
def get_vmess_name(vmess_link):
    if not vmess_link.startswith("vmess://"):
        return None
    try:
        b64_part = vmess_link[8:]
        decoded_str = decode_base64(b64_part)
        if decoded_str:
            vmess_json = json.loads(decoded_str)
            return vmess_json.get('ps')
    except Exception as e:
        logging.warning(f"Failed to parse Vmess name from {vmess_link[:30]}...: {e}")
    return None

def get_ssr_name(ssr_link):
    if not ssr_link.startswith("ssr://"):
        return None
    try:
        b64_part = ssr_link[6:]
        decoded_str = decode_base64(b64_part)
        if not decoded_str:
            return None
        parts = decoded_str.split('/?')
        if len(parts) < 2:
            return None
        params_str = parts[1]
        params = parse_qs(params_str)
        if 'remarks' in params and params['remarks']:
            remarks_b64 = params['remarks'][0]
            return decode_base64(remarks_b64)
    except Exception as e:
        logging.warning(f"Failed to parse SSR name from {ssr_link[:30]}...: {e}")
    return None

# --- New Filter Function ---
def should_filter_config(config):
    if 'i_love_' in config.lower():
        logging.warning(f"Filtering by keyword 'I_Love_': {config[:60]}...")
        return True
    percent25_count = config.count('%25')
    if percent25_count >= MIN_PERCENT25_COUNT:
        logging.warning(f"Filtering by high %25 count ({percent25_count}): {config[:60]}...")
        return True
    if len(config) >= MAX_CONFIG_LENGTH:
        logging.warning(f"Filtering by excessive length ({len(config)}): {config[:60]}...")
        return True
    if '%2525' in config:
        logging.warning(f"Filtering by '%2525' presence: {config[:60]}...")
        return True
    return False

async def fetch_url(session, url):
    try:
        async with session.get(url, timeout=REQUEST_TIMEOUT) as response:
            response.raise_for_status()
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            text_content = ""
            for element in soup.find_all(['pre', 'code', 'p', 'div', 'li', 'span', 'td']):
                text_content += element.get_text(separator='\n', strip=True) + "\n"
            if not text_content:
                text_content = soup.get_text(separator=' ', strip=True)
            logging.info(f"Successfully fetched: {url}")
            return url, text_content
    except Exception as e:
        logging.warning(f"Failed to fetch or process {url}: {e}")
        return url, None

def find_matches(text, categories_data):
    matches = {category: set() for category in categories_data}
    for category, patterns in categories_data.items():
        for pattern_str in patterns:
            if not isinstance(pattern_str, str):
                continue
            try:
                is_protocol_pattern = any(proto_prefix in pattern_str for proto_prefix in [p.lower() + "://" for p in PROTOCOL_CATEGORIES])
                if category in PROTOCOL_CATEGORIES or is_protocol_pattern:
                    pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
                    found = pattern.findall(text)
                    if found:
                        cleaned_found = {item.strip() for item in found if item.strip()}
                        matches[category].update(cleaned_found)
            except re.error as e:
                logging.error(f"Regex error for '{pattern_str}' in category '{category}': {e}")
    return {k: v for k, v in matches.items() if v}


def save_to_file(directory, category_name, items_set):
    if not items_set:
        return False, 0
    file_path = os.path.join(directory, f"{category_name}.txt")
    count = len(items_set)
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            for item in sorted(list(items_set)):
                f.write(f"{item}\n")
        logging.info(f"Saved {count} items to {file_path}")
        return True, count
    except Exception as e:
        logging.error(f"Failed to write file {file_path}: {e}")
        return False, 0

<<<<<<< HEAD
# --- ØªØ§Ø¨Ø¹ generate_simple_readme Ø¨Ø§ Ù‚Ø§Ø¨Ù„ÛŒØª Ù†Ù…Ø§ÛŒØ´ Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ ---
def generate_simple_readme(protocol_counts, country_counts, all_keywords_data, github_repo_path="10ium/ScrapeAndCategorize", github_branch="main"):
=======
# --- ØªØ§Ø¨Ø¹ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ generate_simple_readme ---
def generate_simple_readme(protocol_counts, country_counts, all_keywords_data, github_repo_path="10ium/ScrapeAndCategorize", github_branch="main"):
    """Generates README.md with country flags/codes before country name in the same column."""
>>>>>>> 80b6e0fbf644d44f4576259ce2a8e0a0d617b228
    tz = pytz.timezone('Asia/Tehran')
    now = datetime.now(tz)
    timestamp = now.strftime("%Y-%m-%d %H:%M:%S %Z")

    raw_github_base_url = f"https://raw.githubusercontent.com/{github_repo_path}/refs/heads/{github_branch}/{OUTPUT_DIR}"

    md_content = f"# ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ (Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {timestamp})\n\n"
    md_content += "Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n\n"
    md_content += "**ØªÙˆØ¶ÛŒØ­:** ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ù†Ø§Ù…/Ù¾Ø±Ú†Ù… Ú©Ø´ÙˆØ± (Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù…Ø±Ø² Ú©Ù„Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø®ÙÙâ€ŒÙ‡Ø§) Ø¯Ø± **Ø§Ø³Ù… Ú©Ø§Ù†ÙÛŒÚ¯** Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯. Ø§Ø³Ù… Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø¨ØªØ¯Ø§ Ø§Ø² Ø¨Ø®Ø´ `#` Ù„ÛŒÙ†Ú© Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² Ù†Ø§Ù… Ø¯Ø§Ø®Ù„ÛŒ (Ø¨Ø±Ø§ÛŒ Vmess/SSR) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n\n"
    md_content += "**Ù†Ú©ØªÙ‡:** Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ø´Ø¯Øª URL-Encode Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ (Ø­Ø§ÙˆÛŒ ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ§Ø¯ÛŒ `%25`ØŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ ÛŒØ§ Ø¯Ø§Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø®Ø§Øµ) Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø­Ø°Ù Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.\n\n"

    md_content += "## ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§\n\n"
    if protocol_counts:
        md_content += "| Ù¾Ø±ÙˆØªÚ©Ù„ | ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ | Ù„ÛŒÙ†Ú© |\n"
        md_content += "|---|---|---|\n"
        for category_name, count in sorted(protocol_counts.items()):
            file_link = f"{raw_github_base_url}/{category_name}.txt"
            md_content += f"| {category_name} | {count} | [`{category_name}.txt`]({file_link}) |\n"
    else:
        md_content += "Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù¾Ø±ÙˆØªÚ©Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.\n"
    md_content += "\n"

    md_content += "## ğŸŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ (Ø­Ø§ÙˆÛŒ Ú©Ø§Ù†ÙÛŒÚ¯)\n\n"
    if country_counts:
        # Ù‡Ø¯Ø± Ø¬Ø¯ÙˆÙ„ Ú©Ø´ÙˆØ±Ù‡Ø§ Ø¨Ù‡ Û³ Ø³ØªÙˆÙ† Ø¨Ø§Ø²Ú¯Ø´Øª
        md_content += "| Ú©Ø´ÙˆØ± | ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø±ØªØ¨Ø· | Ù„ÛŒÙ†Ú© |\n"
        md_content += "|---|---|---|\n"
        for country_category_name, count in sorted(country_counts.items()):
<<<<<<< HEAD
            item_to_display_as_flag = ""
            persian_name_str = ""
=======
            # flag_or_code_str Ù†Ù…Ø§ÛŒØ´ Ø¯Ù‡Ù†Ø¯Ù‡ Ú†ÛŒØ²ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø§Ø² Ø§Ù†ØªÙ‡Ø§ÛŒ Ù„ÛŒØ³Øª Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ù‡Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒ Ø´ÙˆØ¯
            # (Ú†Ù‡ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ù¾Ø±Ú†Ù… Ø¨Ø§Ø´Ø¯ Ú†Ù‡ Ú©Ø¯ Ú©Ø´ÙˆØ±)
            flag_or_code_str = ""
>>>>>>> 80b6e0fbf644d44f4576259ce2a8e0a0d617b228

            if country_category_name in all_keywords_data:
                keywords_list = all_keywords_data[country_category_name]
                if keywords_list and isinstance(keywords_list, list):
<<<<<<< HEAD
                    # 1. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù¾Ø±Ú†Ù…/Ú©Ø¯ (Ø¨Ø§ Ù…Ù†Ø·Ù‚ Ù‚Ø¨Ù„ÛŒ)
                    for item in keywords_list:
                        if isinstance(item, str) and (2 <= len(item) <= 7):
                            if not item.isalnum(): # Ø§Ú¯Ø± ØµØ±ÙØ§ Ø­Ø±ÙˆÙ Ùˆ Ø¹Ø¯Ø¯ Ù†Ø¨Ø§Ø´Ø¯ (Ø§Ø­ØªÙ…Ø§Ù„Ø§ Ø§ÛŒÙ…ÙˆØ¬ÛŒ)
                                item_to_display_as_flag = item
                                break
                    if not item_to_display_as_flag and keywords_list: # Fallback
                        potential_last_item = keywords_list[-1]
                        if isinstance(potential_last_item, str) and (1 <= len(potential_last_item) <= 7):
                            item_to_display_as_flag = potential_last_item
                    
                    # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ
                    for item in keywords_list:
                        if isinstance(item, str):
                            if item == item_to_display_as_flag: # Ù†Ø¨Ø§ÛŒØ¯ Ø®ÙˆØ¯ Ù¾Ø±Ú†Ù…/Ú©Ø¯ Ø¨Ø§Ø´Ø¯
                                continue
                            if item.lower() == country_category_name.lower(): # Ù†Ø¨Ø§ÛŒØ¯ Ø®ÙˆØ¯ Ú©Ù„ÛŒØ¯ Ø§ØµÙ„ÛŒ (Ù…Ø¹Ù…ÙˆÙ„Ø§ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ) Ø¨Ø§Ø´Ø¯
                                continue
                            if len(item) in [2,3] and item.isupper() and item.isalpha(): # Ù†Ø¨Ø§ÛŒØ¯ Ú©Ø¯ Ú©ÙˆØªØ§Ù‡ Ú©Ø´ÙˆØ± Ø¨Ø§Ø´Ø¯
                                continue
                            
                            if is_persian_like(item): # ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ ÙØ§Ø±Ø³ÛŒ
                                persian_name_str = item
                                break # Ø§ÙˆÙ„ÛŒÙ† Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø§Ø³Øª
            
            # 3. Ø³Ø§Ø®Øª Ù…ØªÙ† Ù†Ù‡Ø§ÛŒÛŒ Ø¨Ø±Ø§ÛŒ Ø³ØªÙˆÙ† "Ú©Ø´ÙˆØ±"
            display_parts = []
            if item_to_display_as_flag:
                display_parts.append(item_to_display_as_flag)
            
            display_parts.append(country_category_name) # Ù†Ø§Ù… Ø§ØµÙ„ÛŒ (Ú©Ù„ÛŒØ¯)

            if persian_name_str:
                display_parts.append(f"({persian_name_str})")
            
            country_display_text = " ".join(display_parts)
            
            file_link = f"{raw_github_base_url}/{country_category_name}.txt"
            link_text = f"{country_category_name}.txt"
=======
                    # ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ø¢Ø®Ø±ÛŒÙ† Ø¢ÛŒØªÙ… Ø¯Ø± Ù„ÛŒØ³ØªØŒ Ù‡Ù…Ø§Ù† Ú†ÛŒØ²ÛŒ Ø§Ø³Øª Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯ (Ù¾Ø±Ú†Ù… ÛŒØ§ Ú©Ø¯)
                    potential_display_item = keywords_list[-1]
                    # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø·ÙˆÙ„ Ù…Ø¹Ù…ÙˆÙ„ Ù¾Ø±Ú†Ù…â€ŒÙ‡Ø§ ÛŒØ§ Ú©Ø¯Ù‡Ø§ÛŒ Ú©Ø´ÙˆØ±
                    if isinstance(potential_display_item, str) and 1 <= len(potential_display_item) <= 7:
                        flag_or_code_str = potential_display_item

            file_link = f"{raw_github_base_url}/{country_category_name}.txt"
            link_text = f"{country_category_name}.txt" # Ù…ØªÙ† Ù„ÛŒÙ†Ú© ÙÙ‚Ø· Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø§Ø³Øª

            # ØªØ±Ú©ÛŒØ¨ Ù¾Ø±Ú†Ù…/Ú©Ø¯ Ø¨Ø§ Ù†Ø§Ù… Ú©Ø´ÙˆØ± Ø¯Ø± Ø³ØªÙˆÙ† Ø§ÙˆÙ„
            country_display_text = country_category_name
            if flag_or_code_str: # Ø§Ú¯Ø± Ú†ÛŒØ²ÛŒ (Ù¾Ø±Ú†Ù… ÛŒØ§ Ú©Ø¯) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
                country_display_text = f"{flag_or_code_str} {country_category_name}"
            
>>>>>>> 80b6e0fbf644d44f4576259ce2a8e0a0d617b228
            md_content += f"| {country_display_text} | {count} | [`{link_text}`]({file_link}) |\n"
    else:
        md_content += "Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ú©Ø´ÙˆØ±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.\n"
    md_content += "\n"

    try:
        with open(README_FILE, 'w', encoding='utf-8') as f:
            f.write(md_content)
        logging.info(f"Successfully generated {README_FILE}")
    except Exception as e:
        logging.error(f"Failed to write {README_FILE}: {e}")

# ØªØ§Ø¨Ø¹ main Ùˆ Ø¨Ù‚ÛŒÙ‡ ØªÙˆØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø§ÛŒØ¯ Ù…Ø§Ù†Ù†Ø¯ Ù†Ø³Ø®Ù‡ Ù‚Ø¨Ù„ÛŒ Ø¨Ø§Ø´Ù†Ø¯ Ú©Ù‡ all_keywords_data
# Ø±Ø§ Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù…ÛŒâ€ŒÚ©Ø±Ø¯Ù†Ø¯. ÙÙ‚Ø· generate_simple_readme ØªØºÛŒÛŒØ± Ú©Ø±Ø¯Ù‡ Ø§Ø³Øª.
# Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù…Ù„ Ø¨ÙˆØ¯Ù†ØŒ ØªØ§Ø¨Ø¹ main Ø§Ø² Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ Ú©Ù¾ÛŒ Ù…ÛŒâ€ŒØ´ÙˆØ¯.

async def main():
    if not os.path.exists(URLS_FILE) or not os.path.exists(KEYWORDS_FILE):
        logging.critical("Input files not found.")
        return

    with open(URLS_FILE, 'r') as f:
        urls = [line.strip() for line in f if line.strip()]
    with open(KEYWORDS_FILE, 'r', encoding='utf-8') as f:
<<<<<<< HEAD
        categories_data = json.load(f)
=======
        categories_data = json.load(f) # categories_data Ø­Ø§ÙˆÛŒ Ú©Ù„ Ù…Ø­ØªÙˆØ§ÛŒ keywords.json Ø§Ø³Øª
>>>>>>> 80b6e0fbf644d44f4576259ce2a8e0a0d617b228

    protocol_patterns_for_matching = {
        cat: patterns for cat, patterns in categories_data.items() if cat in PROTOCOL_CATEGORIES
    }
    country_keywords_for_naming = {
        cat: patterns for cat, patterns in categories_data.items() if cat not in PROTOCOL_CATEGORIES
    }
    country_category_names = list(country_keywords_for_naming.keys())

    logging.info(f"Loaded {len(urls)} URLs and "
                 f"{len(categories_data)} total categories from keywords.json.")

    tasks = []
    sem = asyncio.Semaphore(CONCURRENT_REQUESTS)
    async def fetch_with_sem(session, url_to_fetch):
        async with sem:
            return await fetch_url(session, url_to_fetch)
    async with aiohttp.ClientSession() as session:
        fetched_pages = await asyncio.gather(*[fetch_with_sem(session, u) for u in urls])

    final_configs_by_country = {cat: set() for cat in country_category_names}
    final_all_protocols = {cat: set() for cat in PROTOCOL_CATEGORIES}

    logging.info("Processing pages for config name association...")
    for url, text in fetched_pages:
        if not text:
            continue

        page_protocol_matches = find_matches(text, protocol_patterns_for_matching)
        all_page_configs_after_filter = set()
        for protocol_cat_name, configs_found in page_protocol_matches.items():
            if protocol_cat_name in PROTOCOL_CATEGORIES:
                for config in configs_found:
                    if should_filter_config(config):
                        continue
                    all_page_configs_after_filter.add(config)
                    final_all_protocols[protocol_cat_name].add(config)

        for config in all_page_configs_after_filter:
            name_to_check = None
            if '#' in config:
                try:
                    potential_name = config.split('#', 1)[1]
                    name_to_check = unquote(potential_name).strip()
                    if not name_to_check: name_to_check = None
                except IndexError: pass

            if not name_to_check:
                if config.startswith('ssr://'): name_to_check = get_ssr_name(config)
                elif config.startswith('vmess://'): name_to_check = get_vmess_name(config)

            if not name_to_check: continue
            
            current_name_to_check_str = name_to_check if isinstance(name_to_check, str) else ""

            for country_name_key, keywords_for_country_list in country_keywords_for_naming.items():
                text_keywords_for_country = []
                if isinstance(keywords_for_country_list, list):
                    for kw in keywords_for_country_list:
<<<<<<< HEAD
                        if isinstance(kw, str):
                            # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§Ø³ØªØŒ Ù†Ù‡ Ù†Ù…Ø§ÛŒØ´ Ù†Ø§Ù… Ø¯Ø± Ø±ÛŒØ¯Ù…ÛŒ
                            # Ù¾Ø³ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ùˆ Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ù†Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ø¬Ø³ØªØ¬Ùˆ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯ Ø§Ú¯Ø± Ù‡Ø¯Ù ÙÙ‚Ø· ØªØ·Ø¨ÛŒÙ‚ Ù†Ø§Ù… Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ÛŒØ§ Ú©Ø¯ Ø§Ø³Øª
                            # Ø¨Ø§ Ø§ÛŒÙ† Ø­Ø§Ù„ØŒ Ù…Ù†Ø·Ù‚ ÙØ¹Ù„ÛŒ Ø´Ù…Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø±ÙˆÛŒ Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ Ù‡Ù… ØªØ·Ø¨ÛŒÙ‚ Ø¯Ù‡Ø¯ Ø§Ú¯Ø± Ø¯Ø± Ø§Ø³Ù… Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ø§Ø´Ø¯
                            # ÙØ¹Ù„Ø§ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø±Ø§ Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ± Ø²ÛŒØ§Ø¯ Ø±Ù‡Ø§ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…ØŒ ØªÙ…Ø±Ú©Ø² Ø±ÙˆÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ø±ÛŒØ¯Ù…ÛŒ Ø§Ø³Øª
                            is_potential_emoji_or_short_code = (1 <= len(kw) <= 7)
                            is_alphanumeric = kw.isalnum()
                            if not (is_potential_emoji_or_short_code and not is_alphanumeric): # Ø§Ú¯Ø± Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ù†ÛŒØ³Øª
                                if not is_persian_like(kw): # Ø§Ú¯Ø± ÙØ§Ø±Ø³ÛŒ Ù‡Ù… Ù†ÛŒØ³Øª
                                     text_keywords_for_country.append(kw)
                                elif kw.lower() == country_name_key.lower(): # Ø§Ú¯Ø± Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ Ú©Ù„ÛŒØ¯ Ø§ØµÙ„ÛŒ ÛŒÚ©ÛŒ Ø§Ø³Øª (Ø¨Ø¹ÛŒØ¯)
                                    text_keywords_for_country.append(kw)
=======
                        # Ø§ÛŒÙ†Ø¬Ø§ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø§Ú¯Ø± Ø¢ÛŒØªÙ… Ú©ÙˆØªØ§Ù‡ Ø¨Ø§Ø´Ø¯ Ùˆ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ø­Ø±ÙˆÙ Ùˆ Ø§Ø¹Ø¯Ø§Ø¯ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ø¨Ø§Ø´Ø¯ Ùˆ Ù†Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ØªÙ†ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯.
                        # Ø§Ú¯Ø± Ø¢ÛŒØªÙ… Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ± Ø¨Ø§Ø´Ø¯ ÛŒØ§ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ø­Ø±ÙˆÙ Ùˆ Ø§Ø¹Ø¯Ø§Ø¯ Ø¨Ø§Ø´Ø¯ØŒ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ù…ØªÙ†ÛŒ Ø¯Ø± Ù†Ø¸Ø± Ú¯Ø±ÙØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
                        if isinstance(kw, str):
                            is_potential_emoji_or_short_code = (1 <= len(kw) <= 7)
                            is_alphanumeric = kw.isalnum()
                            # Ø§Ú¯Ø± Ú©ÙˆØªØ§Ù‡ Ø§Ø³Øª Ùˆ alphanumeric Ù†ÛŒØ³Øª (Ù…Ø«Ù„ ğŸ‡¦ğŸ‡«) ÛŒØ§ Ø§Ú¯Ø± alphanumeric Ø§Ø³Øª ÙˆÙ„ÛŒ Ø·ÙˆÙ„Ø´ Ø¨ÛŒØ´ØªØ± Ø§Ø² Û³ Ø§Ø³Øª (Ù…Ø«Ù„ Afghanistan)
                            # ÛŒØ§ Ø§Ú¯Ø± alphanumeric Ù†ÛŒØ³Øª Ùˆ Ø·ÙˆÙ„Ø´ Ø¨ÛŒØ´ØªØ± Ø§Ø² Û³ Ø§Ø³Øª (Ø¨Ø¹ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡ Ú©Ø´ÙˆØ±)
                            # Ù‡Ø¯Ù Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ú©Ø¯Ù‡Ø§ÛŒ Ø¯ÙˆØ­Ø±ÙÛŒ Ùˆ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ù…Ù„ Ú©Ø´ÙˆØ± Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø±ÛŒÙ… ÙˆÙ„ÛŒ Ø§ÛŒÙ…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ù…ØªÙ† Ø­Ø°Ù Ú©Ù†ÛŒÙ….
                            if not (is_potential_emoji_or_short_code and not is_alphanumeric): # Ø§Ú¯Ø± Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ù†ÛŒØ³ØªØŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                                text_keywords_for_country.append(kw)
>>>>>>> 80b6e0fbf644d44f4576259ce2a8e0a0d617b228


                for keyword in text_keywords_for_country:
                    match_found = False
                    is_abbr = (len(keyword) == 2 or len(keyword) == 3) and re.match(r'^[A-Z]+$', keyword)
                    
                    if is_abbr:
                        pattern = r'\b' + re.escape(keyword) + r'\b'
                        if re.search(pattern, current_name_to_check_str, re.IGNORECASE):
                            match_found = True
                    else:
                        if keyword.lower() in current_name_to_check_str.lower():
                            match_found = True
                    
                    if match_found:
                        final_configs_by_country[country_name_key].add(config)
                        break 
                if match_found: break

    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    logging.info(f"Saving files to directory: {OUTPUT_DIR}")

    protocol_counts = {}
    country_counts = {}

    for category, items in final_all_protocols.items():
        saved, count = save_to_file(OUTPUT_DIR, category, items)
        if saved: protocol_counts[category] = count

    for category, items in final_configs_by_country.items():
        saved, count = save_to_file(OUTPUT_DIR, category, items)
        if saved: country_counts[category] = count
    
    generate_simple_readme(protocol_counts, country_counts, categories_data, 
<<<<<<< HEAD
                           github_repo_path="10ium/ScrapeAndCategorize",
                           github_branch="main")
=======
                           github_repo_path="10ium/ScrapeAndCategorize", # Ù…Ø³ÛŒØ± Ø±ÛŒÙ¾Ø§Ø²ÛŒØªÙˆØ±ÛŒ Ø®ÙˆØ¯ØªØ§Ù†
                           github_branch="main") # Ù†Ø§Ù… Ø¨Ø±Ù†Ú† Ø§ØµÙ„ÛŒ Ø´Ù…Ø§
>>>>>>> 80b6e0fbf644d44f4576259ce2a8e0a0d617b228

    logging.info("--- Script Finished ---")

if __name__ == "__main__":
    asyncio.run(main())
