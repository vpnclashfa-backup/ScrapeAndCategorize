import asyncio
import aiohttp
import json
import re
import logging
from bs4 import BeautifulSoup
import os
import shutil
from datetime import datetime
import pytz
import base64
import urllib.parse

# --- Configuration ---
URLS_FILE = 'urls.txt'
KEYWORDS_FILE = 'keywords.json'
OUTPUT_DIR = 'output_configs'
README_FILE = 'README.md'
REJECTED_LOG_FILE = 'rejected_configs_report.md'
REQUEST_TIMEOUT = 15
CONCURRENT_REQUESTS = 10

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# --- Protocol Categories (Ø¨Ø³ÛŒØ§Ø± Ù…Ù‡Ù…: Ø§ÛŒÙ† Ù„ÛŒØ³Øª Ø¨Ø§ÛŒØ¯ Ø¯Ù‚ÛŒÙ‚Ø§Ù‹ Ø¨Ø§ Ú©Ù„ÛŒØ¯Ù‡Ø§ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„ Ø¯Ø± keywords.json Ø´Ù…Ø§ ÛŒÚ©ÛŒ Ø¨Ø§Ø´Ø¯) ---
PROTOCOL_CATEGORIES = [
    "Vmess", "Vless", "Trojan", "ShadowSocks", "ShadowSocksR",
    "Tuic", "Hysteria2", "WireGuard"
]

# <<<--- ØªØ§Ø¨Ø¹ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ø¨Ø³ÛŒØ§Ø± Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± --->>>
def is_config_valid(config_string_original, proto_prefix_val, min_len=20, max_len=3000, max_overall_percent_char_ratio=0.6, max_specific_percent25_count=10):
    """
    Checks if a config string has a valid structure for its specific protocol.
    Returns (True, None) if valid, or (False, "reason_string") if invalid.
    """
    config_string = config_string_original.strip()
    l = len(config_string)

    if not (min_len <= l <= max_len):
        return False, f"Ø·ÙˆÙ„ Ù†Ø§Ù…Ø¹ØªØ¨Ø± ({l}). Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø±: {min_len}-{max_len}"

    if l > 70 and (config_string.count('%') / l) > max_overall_percent_char_ratio :
        return False, f"ØªØ¹Ø¯Ø§Ø¯ Ø¨Ø³ÛŒØ§Ø± Ø²ÛŒØ§Ø¯ Ú©Ø§Ø±Ø§Ú©ØªØ± % ({config_string.count('%')})"
    if config_string.count('%25') > max_specific_percent25_count:
        return False, f"ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ§Ø¯ ØªÚ©Ø±Ø§Ø± '%25' ({config_string.count('%25')})"

    # proto_prefix_val should already be determined and passed to this function
    if not proto_prefix_val:
        return False, "Ù¾ÛŒØ´ÙˆÙ†Ø¯ Ù¾Ø±ÙˆØªÚ©Ù„ Ø¨Ø±Ø§ÛŒ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù…Ø´Ø®Øµ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª (Ø®Ø·Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ)"

    payload = ""
    if "://" in config_string:
        payload = config_string.split("://", 1)[1]
    else:
        return False, "ÙØ±Ù…Øª URI Ù†Ø§Ù…Ø¹ØªØ¨Ø± (Ø¨Ø¯ÙˆÙ† ://)" # Should not happen if find_matches is correct

    main_payload = payload.split("#", 1)[0] # Ø¨Ø®Ø´ Ø§ØµÙ„ÛŒ Ø¨Ø¯ÙˆÙ† Ù†Ø§Ù…

    # --- ØªØ¹ÛŒÛŒÙ† Ù†Ø§Ù… Ú©Ù„ÛŒØ¯ Ù¾Ø±ÙˆØªÚ©Ù„ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø®Ø·Ø§ ---
    proto_name_key = proto_prefix_val.capitalize() # For messages, e.g. "Vless"
    # Handle specific capitalizations if needed, e.g., from PROTOCOL_CATEGORIES list
    for key_in_list in PROTOCOL_CATEGORIES:
        if key_in_list.lower() == proto_prefix_val:
            proto_name_key = key_in_list
            break


    # --- Ø¨Ø±Ø±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ø³Ø§Ø®ØªØ§Ø±ÛŒ Ù…Ø®ØµÙˆØµ Ù‡Ø± Ù¾Ø±ÙˆØªÚ©Ù„ ---

    if proto_prefix_val == "vless":
        if '@' not in main_payload: return False, f"{proto_name_key}: Ø¹Ù„Ø§Ù…Øª @ ÛŒØ§ÙØª Ù†Ø´Ø¯"
        if not re.search(r':\d{2,5}', main_payload): return False, f"{proto_name_key}: Ù¾ÙˆØ±Øª ÛŒØ§ÙØª Ù†Ø´Ø¯"
        uuid_part = main_payload.split('@', 1)[0]
        uuid_pattern = r'^[a-fA-F0-9]{8}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{12}$'
        if not re.match(uuid_pattern, uuid_part):
            return False, f"{proto_name_key}: UUID Ù…Ø¹ØªØ¨Ø± Ø¯Ø± Ø¨Ø®Ø´ Ú©Ø§Ø±Ø¨Ø± ('{uuid_part}') ÛŒØ§ÙØª Ù†Ø´Ø¯"
        try:
            host_part = main_payload.split('@',1)[1].split(':',1)[0].split('?',1)[0]
            if not host_part or not (re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', host_part) or '.' in host_part):
                return False, f"{proto_name_key}: Ù‡Ø§Ø³Øª Ù†Ø§Ù…Ø¹ØªØ¨Ø± ('{host_part}')"
        except IndexError:
            return False, f"{proto_name_key}: Ø®Ø·Ø§ Ø¯Ø± ØªØ¬Ø²ÛŒÙ‡ Ù‡Ø§Ø³Øª"

    elif proto_prefix_val == "vmess":
        if main_payload.startswith("ey"): # Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Base64 JSON
            try:
                missing_padding = len(main_payload) % 4
                if missing_padding:
                    main_payload_padded = main_payload + '=' * (4 - missing_padding)
                else:
                    main_payload_padded = main_payload
                decoded_json_str = base64.urlsafe_b64decode(main_payload_padded).decode('utf-8')
                vmess_obj = json.loads(decoded_json_str)
                required_keys = ["v", "add", "port", "id", "net"]
                if not all(k in vmess_obj for k in required_keys):
                    return False, f"{proto_name_key} (Base64): ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ {required_keys} Ø¯Ø± JSON ÛŒØ§ÙØª Ù†Ø´Ø¯"
                uuid_pattern = r'^[a-fA-F0-9]{8}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{12}$'
                if "id" in vmess_obj and not re.match(uuid_pattern, str(vmess_obj.get("id", ""))): # Ensure ID is string
                    return False, f"{proto_name_key} (Base64): UUID (id) Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¯Ø± JSON"
            except Exception as e:
                return False, f"{proto_name_key} (Base64): Ø®Ø·Ø§ Ø¯Ø± Ø¯ÛŒÚ©Ø¯/ØªØ¬Ø²ÛŒÙ‡ JSON: {str(e)}"
        else: # ÙØ±Ù…Øª Ù‚Ø¯ÛŒÙ…ÛŒâ€ŒØªØ± VMess
            if '@' not in main_payload: return False, f"{proto_name_key} (non-Base64): @ ÛŒØ§ÙØª Ù†Ø´Ø¯"
            if not re.search(r':\d{2,5}', main_payload): return False, f"{proto_name_key} (non-Base64): Ù¾ÙˆØ±Øª ÛŒØ§ÙØª Ù†Ø´Ø¯"
            uuid_part = main_payload.split('@', 1)[0]
            uuid_pattern = r'^[a-fA-F0-9]{8}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{12}$'
            if not re.match(uuid_pattern, uuid_part):
                return False, f"{proto_name_key} (non-Base64): UUID Ù…Ø¹ØªØ¨Ø± ('{uuid_part}') ÛŒØ§ÙØª Ù†Ø´Ø¯"

    elif proto_prefix_val == "trojan":
        if '@' not in main_payload: return False, f"{proto_name_key}: @ ÛŒØ§ÙØª Ù†Ø´Ø¯"
        if not re.search(r':\d{2,5}', main_payload): return False, f"{proto_name_key}: Ù¾ÙˆØ±Øª ÛŒØ§ÙØª Ù†Ø´Ø¯"
        try:
            host_part = main_payload.split('@',1)[1].split(':',1)[0].split('?',1)[0]
            if not host_part or not (re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', host_part) or '.' in host_part):
                return False, f"{proto_name_key}: Ù‡Ø§Ø³Øª Ù†Ø§Ù…Ø¹ØªØ¨Ø± ('{host_part}')"
        except IndexError:
            return False, f"{proto_name_key}: Ø®Ø·Ø§ Ø¯Ø± ØªØ¬Ø²ÛŒÙ‡ Ù‡Ø§Ø³Øª"

    elif proto_prefix_val == "ss": # ShadowSocks
        if '@' in main_payload and re.search(r':\d{2,5}', main_payload.split('@',1)[-1]):
            pass # Ø³Ø§Ø®ØªØ§Ø± user:pass@host:port Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ù‡ Ù†Ø¸Ø± Ø¯Ø±Ø³Øª Ø§Ø³Øª
        else: # Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ ÙØ±Ù…Øª Base64 Ú©Ø§Ù…Ù„ (SIP002)
            try:
                # Ø¨Ø±Ø§ÛŒ ss://BASE64ØŒ Ú©Ù„ main_payload Ø¨Ø§ÛŒØ¯ Base64 Ø¨Ø§Ø´Ø¯
                decoded_ss_payload = base64.urlsafe_b64decode(main_payload + '=' * (-len(main_payload) % 4)).decode('utf-8')
                # Ù¾Ø³ Ø§Ø² Ø¯ÛŒÚ©Ø¯ Ú©Ø±Ø¯Ù†ØŒ Ø§Ù†ØªØ¸Ø§Ø± Ø¯Ø§Ø±ÛŒÙ… user@host:port ÛŒØ§ Ø³Ø§Ø®ØªØ§Ø± JSON Ø¨Ø¨ÛŒÙ†ÛŒÙ….
                # ÛŒÚ© Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø¯Ù‡: Ø¢ÛŒØ§ @ Ùˆ :Ù¾ÙˆØ±Øª Ø¯Ø± Ø±Ø´ØªÙ‡ Ø¯ÛŒÚ©Ø¯ Ø´Ø¯Ù‡ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯ØŸ
                # Ø§ÛŒÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø±Ø§ÛŒ ÙØ±Ù…Øªâ€ŒÙ‡Ø§ÛŒ SIP002 JSON Ø®ÛŒÙ„ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ù…Ø§ Ø§Ø² Ø±Ø¯ Ú©Ø±Ø¯Ù† Ø¨ÛŒâ€ŒØ¯Ù„ÛŒÙ„ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
                if not ('@' in decoded_ss_payload and re.search(r':\d{2,5}', decoded_ss_payload.split('@',1)[-1])):
                    # Ø§Ú¯Ø± Ø³Ø§Ø®ØªØ§Ø± user@host:port Ù†Ø¨ÙˆØ¯ØŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª JSON Ø¨Ø§Ø´Ø¯.
                    # Ø§ÛŒÙ†Ø¬Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… ÛŒÚ© Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ JSON Ø¨ÙˆØ¯Ù† Ø§Ù†Ø¬Ø§Ù… Ø¯Ù‡ÛŒÙ….
                    try:
                        json.loads(decoded_ss_payload) # Ø¢ÛŒØ§ JSON Ù…Ø¹ØªØ¨Ø± Ø§Ø³ØªØŸ
                        # Ø§Ú¯Ø± JSON Ø¨ÙˆØ¯ØŒ ÙØ¹Ù„Ø§ Ù‚Ø¨ÙˆÙ„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…. Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ± ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ JSON Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø§Ø³Øª.
                    except json.JSONDecodeError:
                        return False, f"{proto_name_key} (Base64): Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø®Ù„ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² Ø¯ÛŒÚ©Ø¯ØŒ Ù†Ù‡ user@host:port Ø§Ø³Øª Ùˆ Ù†Ù‡ JSON Ù…Ø¹ØªØ¨Ø±"
            except Exception:
                 return False, f"{proto_name_key}: ÙØ±Ù…Øª Base64 Ù†Ø§Ù…Ø¹ØªØ¨Ø±"

    elif proto_prefix_val == "ssr":
        try:
            if not isinstance(main_payload, str):
                return False, f"{proto_name_key}: Ø¨Ø®Ø´ Ø§ØµÙ„ÛŒ Base64 ÛŒÚ© Ø±Ø´ØªÙ‡ Ù†ÛŒØ³Øª"
            decoded_ssr_payload = base64.urlsafe_b64decode(main_payload + '=' * (-len(main_payload) % 4)).decode('utf-8')
            parts = decoded_ssr_payload.split(':')
            if len(parts) < 6 : return False, f"{proto_name_key}: Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø®Ù„ÛŒ Base64 Ú©Ù…ØªØ± Ø§Ø² 6 Ø¨Ø®Ø´ Ø¯Ø§Ø±Ø¯"
            if not re.match(r'^\d{1,5}$',parts[1]): return False, f"{proto_name_key}: Ù¾ÙˆØ±Øª ('{parts[1]}') Ø¯Ø± Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø®Ù„ÛŒ Base64 Ù†Ø§Ù…Ø¹ØªØ¨Ø±"
        except Exception as e:
            return False, f"{proto_name_key}: Ø®Ø·Ø§ Ø¯Ø± Ø¯ÛŒÚ©Ø¯ Base64 ÛŒØ§ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø®Ù„ÛŒ: {str(e)}"

    elif proto_prefix_val == "tuic":
        if '@' not in main_payload: return False, f"{proto_name_key}: @ ÛŒØ§ÙØª Ù†Ø´Ø¯"
        if not re.search(r':\d{2,5}', main_payload): return False, f"{proto_name_key}: Ù¾ÙˆØ±Øª ÛŒØ§ÙØª Ù†Ø´Ø¯"
        user_info = main_payload.split('@', 1)[0]
        if ':' not in user_info: return False, f"{proto_name_key}: ÙØ±Ù…Øª 'UUID:Password' Ø¯Ø± Ø¨Ø®Ø´ Ú©Ø§Ø±Ø¨Ø± ('{user_info}') Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø± Ø§Ø³Øª"
        tuic_uuid = user_info.split(':',1)[0]
        uuid_pattern = r'^[a-fA-F0-9]{8}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{4}-?[a-fA-F0-9]{12}$'
        if not re.match(uuid_pattern, tuic_uuid):
            return False, f"{proto_name_key}: UUID Ù…Ø¹ØªØ¨Ø± ('{tuic_uuid}') Ø¯Ø± Ø¨Ø®Ø´ Ú©Ø§Ø±Ø¨Ø± ÛŒØ§ÙØª Ù†Ø´Ø¯"
        try:
            host_part = main_payload.split('@',1)[1].split(':',1)[0].split('?',1)[0]
            if not host_part or not (re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', host_part) or '.' in host_part):
                return False, f"{proto_name_key}: Ù‡Ø§Ø³Øª Ù†Ø§Ù…Ø¹ØªØ¨Ø± ('{host_part}')"
        except IndexError:
            return False, f"{proto_name_key}: Ø®Ø·Ø§ Ø¯Ø± ØªØ¬Ø²ÛŒÙ‡ Ù‡Ø§Ø³Øª"


    elif proto_prefix_val == "hy2":
        if '@' not in main_payload: return False, f"{proto_name_key}: @ ÛŒØ§ÙØª Ù†Ø´Ø¯"
        if not re.search(r':\d{2,5}', main_payload): return False, f"{proto_name_key}: Ù¾ÙˆØ±Øª ÛŒØ§ÙØª Ù†Ø´Ø¯"
        try:
            host_part = main_payload.split('@',1)[1].split(':',1)[0].split('?',1)[0]
            if not host_part or not (re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', host_part) or '.' in host_part):
                return False, f"{proto_name_key}: Ù‡Ø§Ø³Øª Ù†Ø§Ù…Ø¹ØªØ¨Ø± ('{host_part}')"
        except IndexError:
            return False, f"{proto_name_key}: Ø®Ø·Ø§ Ø¯Ø± ØªØ¬Ø²ÛŒÙ‡ Ù‡Ø§Ø³Øª"

    elif proto_prefix_val == "wireguard":
        if '@' not in main_payload: return False, f"{proto_name_key}: @ ÛŒØ§ÙØª Ù†Ø´Ø¯"
        if not re.search(r':\d{2,5}', main_payload): return False, f"{proto_name_key}: Ù¾ÙˆØ±Øª ÛŒØ§ÙØª Ù†Ø´Ø¯"
        query_part = main_payload.split('?', 1)[1] if '?' in main_payload else ""
        if 'publickey=' not in query_part.lower(): return False, f"{proto_name_key}: Ù¾Ø§Ø±Ø§Ù…ØªØ± 'publickey' ÛŒØ§ÙØª Ù†Ø´Ø¯"
        # address= Ù¾Ø§Ø±Ø§Ù…ØªØ± Ù…Ù‡Ù…ÛŒ Ø§Ø³Øª Ø§Ù…Ø§ Ø¯Ø± Ø¨Ø±Ø®ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯ Ù‡Ø§ÛŒ ÙˆØ§ÛŒØ±Ú¯Ø§Ø±Ø¯ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± Ø¨Ø®Ø´ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¨Ø§Ø´Ø¯ ÛŒØ§ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø³Ø±ÙˆØ± ØªØ¹ÛŒÛŒÙ† Ø´ÙˆØ¯
        # if 'address=' not in query_part.lower(): return False, f"{proto_name_key}: Ù¾Ø§Ø±Ø§Ù…ØªØ± 'address' ÛŒØ§ÙØª Ù†Ø´Ø¯"
        try:
            host_part = main_payload.split('@',1)[1].split(':',1)[0].split('?',1)[0]
            if not host_part or not (re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', host_part) or '.' in host_part):
                return False, f"{proto_name_key}: Ù‡Ø§Ø³Øª Ù†Ø§Ù…Ø¹ØªØ¨Ø± ('{host_part}')"
        except IndexError:
            return False, f"{proto_name_key}: Ø®Ø·Ø§ Ø¯Ø± ØªØ¬Ø²ÛŒÙ‡ Ù‡Ø§Ø³Øª"

    return True, None
# <<<--- Ù¾Ø§ÛŒØ§Ù† ØªØ§Ø¨Ø¹ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ --->>>

async def fetch_url(session, url):
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        async with session.get(url, timeout=REQUEST_TIMEOUT, headers=headers) as response:
            response.raise_for_status()
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            text = soup.get_text(separator=' ', strip=True)
            logging.info(f"Successfully fetched: {url}")
            return url, text
    except Exception as e:
        logging.warning(f"Failed to fetch or process {url}: {e}")
        return url, None

def find_matches(text, categories):
    matches = {category: set() for category in categories}
    for category, patterns in categories.items():
        for pattern_str in patterns:
            try:
                pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
                found = pattern.findall(text)
                if found:
                    matches[category].update(found)
            except re.error as e:
                logging.error(f"Regex error for '{pattern_str}': {e}")
    return {k: v for k, v in matches.items() if v}

def save_to_file(directory, category_name, items_set):
    if not items_set:
        return False, 0
    file_path = os.path.join(directory, f"{category_name}.txt")
    count = len(items_set)
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            for item in sorted(list(items_set)):
                f.write(f"{item}\n")
        logging.info(f"Saved {count} items to {file_path}")
        return True, count
    except Exception as e:
        logging.error(f"Failed to write file {file_path}: {e}")
        return False, 0

def save_rejected_log(rejected_items):
    tz = pytz.timezone('Asia/Tehran')
    now = datetime.now(tz)
    timestamp = now.strftime("%Y-%m-%d %H:%M:%S %Z")

    md_content = f"# âš ï¸ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ø±Ø¯ Ø´Ø¯Ù‡ (Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {timestamp})\n\n"
    md_content += "Ø¯Ø± Ø§ÛŒÙ† Ú¯Ø²Ø§Ø±Ø´ØŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ ØªÙˆØ³Ø· Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ù…Ø¹ØªØ¨Ø± ØªØ´Ø®ÛŒØµ Ø¯Ø§Ø¯Ù‡ Ù†Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ø¯Ù„ÛŒÙ„ Ø±Ø¯ Ø´Ø¯Ù† Ùˆ URL Ù…Ù†Ø¨Ø¹ Ù„ÛŒØ³Øª Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.\n\n"

    if not rejected_items:
        md_content += "Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ÛŒ Ø¯Ø± Ø§ÛŒÙ† Ø§Ø¬Ø±Ø§ Ø±Ø¯ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.\n"
    else:
        for item in rejected_items:
            config = item["config"]
            reason = item["reason"]
            source_url = item["url"]
            md_content += f"## Ú©Ø§Ù†ÙÛŒÚ¯:\n```text\n{config}\n```\n"
            md_content += f"**Ø¯Ù„ÛŒÙ„ Ø±Ø¯ Ø´Ø¯Ù†:** {reason}\n\n"
            md_content += f"**Ù…Ù†Ø¨Ø¹ URL:** `{source_url}`\n\n"
            md_content += "---\n\n"

    try:
        with open(REJECTED_LOG_FILE, 'w', encoding='utf-8') as f:
            f.write(md_content)
        logging.info(f"Generated {REJECTED_LOG_FILE} with {len(rejected_items)} entries.")
    except Exception as e:
        logging.error(f"Failed to write {REJECTED_LOG_FILE}: {e}")


def generate_simple_readme(protocol_counts, country_counts):
    tz = pytz.timezone('Asia/Tehran')
    now = datetime.now(tz)
    timestamp = now.strftime("%Y-%m-%d %H:%M:%S %Z")

    md_content = f"# ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ (Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {timestamp})\n\n"
    md_content += "Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n\n"
    md_content += f"**ØªÙˆØ¶ÛŒØ­:** ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ù†Ø§Ù…/Ù¾Ø±Ú†Ù… Ú©Ø´ÙˆØ± (Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù…Ø±Ø² Ú©Ù„Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø®ÙÙâ€ŒÙ‡Ø§) Ø¯Ø± **Ø§Ø³Ù… Ø®ÙˆØ¯ Ú©Ø§Ù†ÙÛŒÚ¯ (Ø¨Ø¹Ø¯ Ø§Ø² #)** Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯. Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø² Ù†Ø¸Ø± Ø³Ø§Ø®ØªØ§Ø±ÛŒ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯. Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ø±Ø¯ Ø´Ø¯Ù‡ Ø±Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¯Ø± [`{REJECTED_LOG_FILE}`](./{REJECTED_LOG_FILE}) Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\n\n"

    md_content += "## ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§\n\n"
    if protocol_counts:
        md_content += "| Ù¾Ø±ÙˆØªÚ©Ù„ | ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ | Ù„ÛŒÙ†Ú© |\n"
        md_content += "|---|---|---|\n"
        for category, count in sorted(protocol_counts.items()):
            md_content += f"| {category} | {count} | [`{category}.txt`](./{OUTPUT_DIR}/{category}.txt) |\n"
    else:
        md_content += "Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù¾Ø±ÙˆØªÚ©Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.\n"
    md_content += "\n"

    md_content += "## ğŸŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ (Ø­Ø§ÙˆÛŒ Ú©Ø§Ù†ÙÛŒÚ¯)\n\n"
    if country_counts:
        md_content += "| Ú©Ø´ÙˆØ± | ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø±ØªØ¨Ø· | Ù„ÛŒÙ†Ú© |\n"
        md_content += "|---|---|---|\n"
        for category, count in sorted(country_counts.items()):
            md_content += f"| {category} | {count} | [`{category}.txt`](./{OUTPUT_DIR}/{category}.txt) |\n"
    else:
        md_content += "Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ú©Ø´ÙˆØ±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.\n"
    md_content += "\n"

    try:
        with open(README_FILE, 'w', encoding='utf-8') as f:
            f.write(md_content)
        logging.info(f"Successfully generated {README_FILE}")
    except Exception as e:
        logging.error(f"Failed to write {README_FILE}: {e}")


async def main():
    if not os.path.exists(URLS_FILE) or not os.path.exists(KEYWORDS_FILE):
        logging.critical("Input files not found.")
        return

    with open(URLS_FILE, 'r') as f:
        urls = [line.strip() for line in f if line.strip()]
    with open(KEYWORDS_FILE, 'r', encoding='utf-8') as f:
        categories = json.load(f)

    country_categories = {cat: keywords for cat, keywords in categories.items() if cat not in PROTOCOL_CATEGORIES}
    country_category_names = list(country_categories.keys())

    logging.info(f"Loaded {len(urls)} URLs and "
                 f"{len(categories)} categories.")

    tasks = []
    sem = asyncio.Semaphore(CONCURRENT_REQUESTS)
    async def fetch_with_sem(session, url):
        async with sem:
            return await fetch_url(session, url)
    async with aiohttp.ClientSession() as session:
        fetched_pages = await asyncio.gather(*[fetch_with_sem(session, url) for url in urls])

    final_configs_by_country = {cat: set() for cat in country_category_names}
    final_all_protocols = {cat_key: set() for cat_key in PROTOCOL_CATEGORIES} # Use keys from list
    rejected_configs_log = []

    logging.info("Processing pages & filtering configs...")
    for url, text in fetched_pages:
        if not text:
            continue

        page_matches = find_matches(text, categories)

        all_page_configs_found_by_regex = set()
        for cat_key in PROTOCOL_CATEGORIES:
            if cat_key in page_matches:
                all_page_configs_found_by_regex.update(page_matches[cat_key])

        for config in all_page_configs_found_by_regex:
            current_proto_prefix_val = None
            # Determine the protocol prefix based on PROTOCOL_CATEGORIES
            for p_key_check in PROTOCOL_CATEGORIES:
                if config.lower().startswith(p_key_check.lower() + "://"):
                    current_proto_prefix_val = p_key_check.lower()
                    break
            
            # Pass the original config and determined prefix to validation
            is_valid, reason = is_config_valid(config, current_proto_prefix_val)

            if not is_valid:
                rejected_configs_log.append({"config": config, "reason": reason, "url": url})
                # logging.warning(f"REJECTED ('{reason}'): {config[:70]}... (URL: {url})") # Logged inside is_config_valid
                continue

            # Add to its main protocol list using the original key from PROTOCOL_CATEGORIES
            actual_protocol_category_key = None
            if current_proto_prefix_val: # Make sure a prefix was actually found
                for p_key_main in PROTOCOL_CATEGORIES:
                    if current_proto_prefix_val == p_key_main.lower():
                        actual_protocol_category_key = p_key_main
                        break
            
            if actual_protocol_category_key:
                 final_all_protocols[actual_protocol_category_key].add(config)
            
            # Associate with country if name matches
            if '#' in config:
                try:
                    name_part = config.split('#', 1)[1]
                except IndexError:
                    continue

                for country, keywords in country_categories.items():
                    for keyword in keywords:
                        match_found = False
                        is_abbr = (len(keyword) == 2 or len(keyword) == 3) and re.match(r'^[A-Z]+$', keyword)

                        if is_abbr:
                            pattern = r'\b' + re.escape(keyword) + r'\b'
                            if re.search(pattern, name_part, re.IGNORECASE):
                                match_found = True
                        else:
                            if keyword.lower() in name_part.lower():
                                match_found = True

                        if match_found:
                            # DEBUG for specific country
                            # if country == "Bangladesh":
                            #    logging.warning(f"DEBUG: Adding '{config}' to 'Bangladesh' because keyword '{keyword}' matched name '{name_part}'.")
                            final_configs_by_country[country].add(config)
                            break
    # --- Save Output Files ---
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    logging.info(f"Saving files to directory: {OUTPUT_DIR}")

    protocol_counts = {}
    country_counts = {}

    for category, items in final_all_protocols.items():
        saved, count = save_to_file(OUTPUT_DIR, category, items)
        if saved: protocol_counts[category] = count

    for category, items in final_configs_by_country.items():
        saved, count = save_to_file(OUTPUT_DIR, category, items)
        if saved: country_counts[category] = count

    # --- Generate README.md & Rejection Log ---
    generate_simple_readme(protocol_counts, country_counts)
    save_rejected_log(rejected_configs_log)

    logging.info("--- Script Finished ---")

if __name__ == "__main__":
    asyncio.run(main())
