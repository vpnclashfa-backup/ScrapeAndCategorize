import asyncio
import aiohttp
import json
import re
import logging
from bs4 import BeautifulSoup
import os
import shutil
from datetime import datetime
import pytz
import base64
from urllib.parse import parse_qs, unquote # <--- unquote Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯

# --- Configuration ---
URLS_FILE = 'urls.txt'
KEYWORDS_FILE = 'keywords.json'
OUTPUT_DIR = 'output_configs'
README_FILE = 'README.md'
REQUEST_TIMEOUT = 15  # seconds
CONCURRENT_REQUESTS = 10  # Max concurrent requests

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# --- Protocol Categories ---
PROTOCOL_CATEGORIES = [
    "Vmess", "Vless", "Trojan", "ShadowSocks", "ShadowSocksR",
    "Tuic", "Hysteria2", "WireGuard"
]

# --- Base64 Decoding Helper ---
def decode_base64(data):
    """
    Decodes a Base64 string, handling URL-safe and padding issues.
    Tries URL-safe first, then standard. Returns None on error.
    """
    try:
        data = data.replace('_', '/').replace('-', '+') # Ensure standard alphabet
        missing_padding = len(data) % 4
        if missing_padding:
            data += '=' * (4 - missing_padding)
        return base64.b64decode(data).decode('utf-8')
    except Exception as e:
        # logging.debug(f"Base64 decode failed for '{data[:20]}...': {e}")
        return None

# --- Protocol Name Extraction Helpers ---
def get_vmess_name(vmess_link):
    """Extracts the name (ps) from a Vmess link if possible."""
    if not vmess_link.startswith("vmess://"):
        return None
    try:
        b64_part = vmess_link[8:]
        decoded_str = decode_base64(b64_part)
        if decoded_str:
            vmess_json = json.loads(decoded_str)
            return vmess_json.get('ps')
    except Exception as e:
        logging.warning(f"Failed to parse Vmess name from {vmess_link[:30]}...: {e}")
    return None

def get_ssr_name(ssr_link):
    """Extracts the name (remarks) from an SSR link if possible."""
    if not ssr_link.startswith("ssr://"):
        return None
    try:
        b64_part = ssr_link[6:]
        decoded_str = decode_base64(b64_part)
        if not decoded_str:
            return None

        parts = decoded_str.split('/?')
        if len(parts) < 2:
            return None

        params_str = parts[1]
        params = parse_qs(params_str)

        if 'remarks' in params and params['remarks']:
            remarks_b64 = params['remarks'][0]
            return decode_base64(remarks_b64) # Remarks is also Base64
    except Exception as e:
        logging.warning(f"Failed to parse SSR name from {ssr_link[:30]}...: {e}")
    return None

async def fetch_url(session, url):
    """Asynchronously fetches the content of a single URL."""
    try:
        async with session.get(url, timeout=REQUEST_TIMEOUT) as response:
            response.raise_for_status()
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            text_content = ""
            for element in soup.find_all(['pre', 'code', 'p', 'div', 'li', 'span', 'td']):
                 text_content += element.get_text(separator='\n', strip=True) + "\n"
            if not text_content:
                text_content = soup.get_text(separator=' ', strip=True)

            logging.info(f"Successfully fetched: {url}")
            return url, text_content
    except Exception as e:
        logging.warning(f"Failed to fetch or process {url}: {e}")
        return url, None

def find_matches(text, categories):
    """Finds all matches using keywords.json patterns."""
    matches = {category: set() for category in categories}
    for category, patterns in categories.items():
        for pattern_str in patterns:
            try:
                pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
                found = pattern.findall(text)
                if found:
                    cleaned_found = {item.strip() for item in found if item.strip()}
                    matches[category].update(cleaned_found)
            except re.error as e:
                logging.error(f"Regex error for '{pattern_str}': {e}")
    return {k: v for k, v in matches.items() if v}

def save_to_file(directory, category_name, items_set):
    """Helper function to save a set to a file and return count."""
    if not items_set:
        return False, 0
    file_path = os.path.join(directory, f"{category_name}.txt")
    count = len(items_set)
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            for item in sorted(list(items_set)):
                f.write(f"{item}\n")
        logging.info(f"Saved {count} items to {file_path}")
        return True, count
    except Exception as e:
        logging.error(f"Failed to write file {file_path}: {e}")
        return False, 0

def generate_simple_readme(protocol_counts, country_counts):
    """Generates a simpler README.md content."""
    tz = pytz.timezone('Asia/Tehran')
    now = datetime.now(tz)
    timestamp = now.strftime("%Y-%m-%d %H:%M:%S %Z")

    md_content = f"# ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ (Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {timestamp})\n\n"
    md_content += "Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n\n"
    md_content += "**ØªÙˆØ¶ÛŒØ­:** ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ù†Ø§Ù…/Ù¾Ø±Ú†Ù… Ú©Ø´ÙˆØ± (Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù…Ø±Ø² Ú©Ù„Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø®ÙÙâ€ŒÙ‡Ø§) Ø¯Ø± **Ø§Ø³Ù… Ú©Ø§Ù†ÙÛŒÚ¯** Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯. Ø§Ø³Ù… Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø¨ØªØ¯Ø§ Ø§Ø² Ø¨Ø®Ø´ `#` Ù„ÛŒÙ†Ú© Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² Ù†Ø§Ù… Ø¯Ø§Ø®Ù„ÛŒ (Ø¨Ø±Ø§ÛŒ Vmess/SSR) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n\n"
    md_content += "**Ù†Ú©ØªÙ‡:** Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ø´Ø¯Øª URL-Encode Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ (Ø­Ø§ÙˆÛŒ `%25%25%25` ÛŒØ§ `I_Love_`) Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø­Ø°Ù Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.\n\n" # <--- Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯

    md_content += "## ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§\n\n"
    if protocol_counts:
        md_content += "| Ù¾Ø±ÙˆØªÚ©Ù„ | ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ | Ù„ÛŒÙ†Ú© |\n"
        md_content += "|---|---|---|\n"
        for category, count in sorted(protocol_counts.items()):
            md_content += f"| {category} | {count} | [`{category}.txt`](./{OUTPUT_DIR}/{category}.txt) |\n"
    else:
        md_content += "Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù¾Ø±ÙˆØªÚ©Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.\n"
    md_content += "\n"

    md_content += "## ğŸŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ (Ø­Ø§ÙˆÛŒ Ú©Ø§Ù†ÙÛŒÚ¯)\n\n"
    if country_counts:
        md_content += "| Ú©Ø´ÙˆØ± | ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø±ØªØ¨Ø· | Ù„ÛŒÙ†Ú© |\n"
        md_content += "|---|---|---|\n"
        for category, count in sorted(country_counts.items()):
            md_content += f"| {category} | {count} | [`{category}.txt`](./{OUTPUT_DIR}/{category}.txt) |\n"
    else:
        md_content += "Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ú©Ø´ÙˆØ±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.\n"
    md_content += "\n"

    try:
        with open(README_FILE, 'w', encoding='utf-8') as f:
            f.write(md_content)
        logging.info(f"Successfully generated {README_FILE}")
    except Exception as e:
        logging.error(f"Failed to write {README_FILE}: {e}")


async def main():
    """Main function to coordinate the scraping process."""
    if not os.path.exists(URLS_FILE) or not os.path.exists(KEYWORDS_FILE):
        logging.critical("Input files not found.")
        return

    with open(URLS_FILE, 'r') as f:
        urls = [line.strip() for line in f if line.strip()]
    with open(KEYWORDS_FILE, 'r', encoding='utf-8') as f:
        categories = json.load(f)

    country_categories = {cat: keywords for cat, keywords in categories.items() if cat not in PROTOCOL_CATEGORIES}
    country_category_names = list(country_categories.keys())

    logging.info(f"Loaded {len(urls)} URLs and "
                 f"{len(categories)} categories.")

    # --- Fetch URLs ---
    tasks = []
    sem = asyncio.Semaphore(CONCURRENT_REQUESTS)
    async def fetch_with_sem(session, url):
        async with sem:
            return await fetch_url(session, url)
    async with aiohttp.ClientSession() as session:
        fetched_pages = await asyncio.gather(*[fetch_with_sem(session, url) for url in urls])

    # --- Process & Aggregate ---
    final_configs_by_country = {cat: set() for cat in country_category_names}
    final_all_protocols = {cat: set() for cat in PROTOCOL_CATEGORIES}

    # <<<--- Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯: Ø§Ù„Ú¯ÙˆÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† --- >>>
    filter_pattern = re.compile(r'(%25){3,}') # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Û³ ÛŒØ§ Ø¨ÛŒØ´ØªØ± %25 Ù¾Ø´Øª Ø³Ø± Ù‡Ù…
    # <<<--- Ù¾Ø§ÛŒØ§Ù† Ø¨Ø®Ø´ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯Ù‡ --- >>>

    logging.info("Processing pages for config name association...")
    for url, text in fetched_pages:
        if not text:
            continue

        page_matches = find_matches(text, categories)

        all_page_configs = set()
        for cat in PROTOCOL_CATEGORIES:
            if cat in page_matches:
                # <<<--- ØªØºÛŒÛŒØ± Ù…Ù‡Ù…: Ø§Ø¹Ù…Ø§Ù„ ÙÛŒÙ„ØªØ± --- >>>
                for config in page_matches[cat]:
                    # Ø§Ú¯Ø± Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ø§ Ø§Ù„Ú¯ÙˆÛŒ ÙÛŒÙ„ØªØ± Ù…Ø·Ø§Ø¨Ù‚Øª Ø¯Ø§Ø´Øª ÛŒØ§ Ø­Ø§ÙˆÛŒ I_Love_ Ø¨ÙˆØ¯ØŒ Ø¢Ù† Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ø¨Ú¯ÛŒØ±
                    if filter_pattern.search(config) or 'I_Love_' in config:
                        logging.warning(f"Skipping heavily encoded/filtered config: {config[:60]}...")
                        continue # Ø¨Ø±Ùˆ Ø³Ø±Ø§Øº Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ø¹Ø¯ÛŒ Ùˆ Ø§ÛŒÙ† ÛŒÚ©ÛŒ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ù†Ú©Ù†

                    # Ø§Ú¯Ø± ÙÛŒÙ„ØªØ± Ù†Ø´Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                    all_page_configs.add(config)
                    final_all_protocols[cat].add(config)
                # <<<--- Ù¾Ø§ÛŒØ§Ù† ØªØºÛŒÛŒØ± Ù…Ù‡Ù… --- >>>


        # Ø­Ø§Ù„Ø§ Ø¨Ø§ all_page_configs Ú©Ù‡ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡ Ø§Ø³ØªØŒ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        for config in all_page_configs:
            name_to_check = None

            # 1. Ø§ÙˆÙ„ÙˆÛŒØª Ø¨Ø§ Ù†Ø§Ù… Ø¨Ø¹Ø¯ Ø§Ø² #
            if '#' in config:
                try:
                    potential_name = config.split('#', 1)[1]
                    name_to_check = unquote(potential_name).strip()
                    if not name_to_check:
                        name_to_check = None
                except IndexError:
                    pass

            # 2. Ø§Ú¯Ø± Ù†Ø§Ù… # Ù†Ø¨ÙˆØ¯ØŒ Ù†Ø§Ù… Ø¯Ø§Ø®Ù„ÛŒ Ø±Ø§ Ú†Ú© Ú©Ù†
            if not name_to_check:
                if config.startswith('ssr://'):
                    name_to_check = get_ssr_name(config)
                elif config.startswith('vmess://'):
                    name_to_check = get_vmess_name(config)

            # 3. Ø§Ú¯Ø± Ù†Ø§Ù…ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø¨Ø±Ùˆ Ø¨Ø¹Ø¯ÛŒ
            if not name_to_check:
                continue

            # 4. Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´ÙˆØ± Ø¨Ø§ Ù†Ø§Ù… Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡
            for country, keywords in country_categories.items():
                for keyword in keywords:
                    match_found = False
                    is_abbr = (len(keyword) == 2 or len(keyword) == 3) and re.match(r'^[A-Z]+$', keyword)

                    if is_abbr:
                        pattern = r'\b' + re.escape(keyword) + r'\b'
                        if re.search(pattern, name_to_check, re.IGNORECASE):
                            match_found = True
                    else:
                        if keyword.lower() in name_to_check.lower():
                            match_found = True

                    if match_found:
                        final_configs_by_country[country].add(config)
                        break
                if match_found:
                    break


    # --- Save Output Files ---
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    logging.info(f"Saving files to directory: {OUTPUT_DIR}")

    protocol_counts = {}
    country_counts = {}

    for category, items in final_all_protocols.items():
        saved, count = save_to_file(OUTPUT_DIR, category, items)
        if saved: protocol_counts[category] = count

    for category, items in final_configs_by_country.items():
        saved, count = save_to_file(OUTPUT_DIR, category, items)
        if saved: country_counts[category] = count

    # --- Generate README.md ---
    generate_simple_readme(protocol_counts, country_counts)

    logging.info("--- Script Finished ---")

if __name__ == "__main__":
    asyncio.run(main())
