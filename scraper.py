import asyncio
import aiohttp
import json
import re
import logging
from bs4 import BeautifulSoup
import os
import shutil
from datetime import datetime
import pytz
import base64
from urllib.parse import parse_qs, unquote
import time # Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ù‡ Ø¯Ø± Ù¾Ø§Ø³Ø® Ù‚Ø¨Ù„ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø´Ø¯ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)

# --- Configuration ---
URLS_FILE = 'urls.txt'
KEYWORDS_FILE = 'keywords.json'
OUTPUT_DIR = 'output_configs'
README_FILE = 'README.md'
REQUEST_TIMEOUT = 15  # seconds
CONCURRENT_REQUESTS = 10  # Max concurrent requests
MAX_CONFIG_LENGTH = 1500
MIN_PERCENT25_COUNT = 15

# --- Logging Setup ---
logging.basicConfig(level=logging.INFO,
                    format='%(asctime)s - %(levelname)s - %(message)s')

# --- Protocol Categories ---
PROTOCOL_CATEGORIES = [
    "Vmess", "Vless", "Trojan", "ShadowSocks", "ShadowSocksR",
    "Tuic", "Hysteria2", "WireGuard"
]

DEFAULT_FLAG = "ğŸ³ï¸"

# --- Base64 Decoding Helper ---
def decode_base64(data):
    try:
        data = data.replace('_', '/').replace('-', '+')
        missing_padding = len(data) % 4
        if missing_padding:
            data += '=' * (4 - missing_padding)
        return base64.b64decode(data).decode('utf-8')
    except Exception:
        return None

# --- Protocol Name Extraction Helpers ---
def get_vmess_name(vmess_link):
    if not vmess_link.startswith("vmess://"):
        return None
    try:
        b64_part = vmess_link[8:]
        decoded_str = decode_base64(b64_part)
        if decoded_str:
            vmess_json = json.loads(decoded_str)
            return vmess_json.get('ps')
    except Exception as e:
        logging.debug(f"Failed to parse Vmess name from {vmess_link[:30]}...: {e}") # Debug level for less noise
    return None

def get_ssr_name(ssr_link):
    if not ssr_link.startswith("ssr://"):
        return None
    try:
        b64_part = ssr_link[6:]
        decoded_str = decode_base64(b64_part)
        if not decoded_str: return None
        parts = decoded_str.split('/?')
        if len(parts) < 2: return None
        params = parse_qs(parts[1])
        if 'remarks' in params and params['remarks']:
            remarks_b64 = params['remarks'][0]
            return decode_base64(remarks_b64)
    except Exception as e:
        logging.debug(f"Failed to parse SSR name from {ssr_link[:30]}...: {e}") # Debug level
    return None

# --- Filter Function (Ø¨Ø§ Ù¾Ø§Ø±Ø§Ù…ØªØ± source_url) ---
def should_filter_config(config, source_url="Unknown source"): # <--- Ù¾Ø§Ø±Ø§Ù…ØªØ± source_url Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
    """
    Checks if a config should be filtered based on heavy encoding,
    specific keywords, or excessive length. Logs the source URL if filtered.
    """
    # 1. Check for specific keywords (case-insensitive)
    if 'i_love_' in config.lower():
        logging.warning(f"Filtering by keyword 'I_Love_' from {source_url}: {config[:60]}...") # <--- source_url Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        return True

    # 2. Check for high count of '%25'
    percent25_count = config.count('%25')
    if percent25_count >= MIN_PERCENT25_COUNT:
        logging.warning(f"Filtering by high %25 count ({percent25_count}) from {source_url}: {config[:60]}...") # <--- source_url Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        return True

    # 3. Check for excessive length
    if len(config) >= MAX_CONFIG_LENGTH:
        logging.warning(f"Filtering by excessive length ({len(config)}) from {source_url}: {config[:60]}...") # <--- source_url Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        return True

    # 4. Check for '%2525' as another indicator
    if '%2525' in config:
        logging.warning(f"Filtering by '%2525' presence from {source_url}: {config[:60]}...") # <--- source_url Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯
        return True

    return False

async def fetch_url(session, url):
    """Asynchronously fetches the content of a single URL."""
    try:
        async with session.get(url, timeout=REQUEST_TIMEOUT) as response:
            response.raise_for_status()
            html = await response.text()
            soup = BeautifulSoup(html, 'html.parser')
            text_content = ""
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø±Ø§ÛŒØ¬â€ŒØªØ± Ùˆ Ù‡Ù…Ú†Ù†ÛŒÙ† div Ùˆ span Ùˆ td
            for element in soup.find_all(['pre', 'code', 'p', 'div', 'li', 'span', 'td', 'article', 'section']):
                text_content += element.get_text(separator='\n', strip=True) + "\n"
            if not text_content.strip(): # Ø§Ú¯Ø± Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø² ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ø®Ø§Øµ Ø®Ø§Ù„ÛŒ Ø¨ÙˆØ¯
                text_content = soup.get_text(separator=' ', strip=True) # Ù…ØªÙ† Ú©Ù„ÛŒ ØµÙØ­Ù‡ Ø±Ø§ Ø¨Ú¯ÛŒØ±

            logging.info(f"Successfully fetched: {url}")
            return url, text_content
    except asyncio.TimeoutError:
        logging.warning(f"Timeout while fetching {url} after {REQUEST_TIMEOUT} seconds.")
        return url, None
    except aiohttp.ClientError as e:
        logging.warning(f"ClientError while fetching {url}: {e}")
        return url, None
    except Exception as e:
        logging.warning(f"Failed to fetch or process {url}: {e}")
        return url, None

def find_matches(text, categories_with_patterns):
    matches = {category: set() for category in categories_with_patterns}
    for category, patterns in categories_with_patterns.items():
        for pattern_str in patterns:
            # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø®ÙˆØ¯ Ø§ÛŒÙ…ÙˆØ¬ÛŒ Ù¾Ø±Ú†Ù… Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø§Ù„Ú¯ÙˆÛŒ regex
            if len(pattern_str) < 5 and any(0x1F1E6 <= ord(char) <= 0x1F1FF for char in pattern_str): # Heuristic for regional indicators
                 continue 

            try:
                if category in PROTOCOL_CATEGORIES: # ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ Ø§Ø² regex Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø´ÙˆØ¯
                    pattern = re.compile(pattern_str, re.IGNORECASE | re.MULTILINE)
                    found = pattern.findall(text)
                    if found:
                        cleaned_found = {item.strip() for item in found if item.strip()}
                        matches[category].update(cleaned_found)
            except re.error as e:
                logging.error(f"Regex error for pattern '{pattern_str}' in category '{category}': {e}")
    return {k: v for k, v in matches.items() if v}


def save_to_file(directory, category_name, items_set):
    if not items_set:
        return False, 0
    file_path = os.path.join(directory, f"{category_name}.txt")
    count = len(items_set)
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            for item in sorted(list(items_set)): # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ ÛŒÚ©Ù†ÙˆØ§Ø®Øª
                f.write(f"{item}\n")
        logging.info(f"Saved {count} items to {file_path}")
        return True, count
    except Exception as e:
        logging.error(f"Failed to write file {file_path}: {e}")
        return False, 0

def generate_simple_readme(protocol_counts, country_counts, country_flags_map):
    tz = pytz.timezone('Asia/Tehran')
    now = datetime.now(tz)
    timestamp = now.strftime("%Y-%m-%d %H:%M:%S %Z")

    md_content = f"# ğŸ“Š Ù†ØªØ§ÛŒØ¬ Ø§Ø³ØªØ®Ø±Ø§Ø¬ (Ø¢Ø®Ø±ÛŒÙ† Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ: {timestamp})\n\n"
    md_content += "Ø§ÛŒÙ† ÙØ§ÛŒÙ„ Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø§ÛŒØ¬Ø§Ø¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.\n\n"
    md_content += "**ØªÙˆØ¶ÛŒØ­:** ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ ÙÙ‚Ø· Ø´Ø§Ù…Ù„ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒÛŒ Ù‡Ø³ØªÙ†Ø¯ Ú©Ù‡ Ù†Ø§Ù…/Ù¾Ø±Ú†Ù… Ú©Ø´ÙˆØ± (Ø¨Ø§ Ø±Ø¹Ø§ÛŒØª Ù…Ø±Ø² Ú©Ù„Ù…Ù‡ Ø¨Ø±Ø§ÛŒ Ù…Ø®ÙÙâ€ŒÙ‡Ø§) Ø¯Ø± **Ø§Ø³Ù… Ú©Ø§Ù†ÙÛŒÚ¯** Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯. Ø§Ø³Ù… Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ø¨ØªØ¯Ø§ Ø§Ø² Ø¨Ø®Ø´ `#` Ù„ÛŒÙ†Ú© Ùˆ Ø¯Ø± ØµÙˆØ±Øª Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² Ù†Ø§Ù… Ø¯Ø§Ø®Ù„ÛŒ (Ø¨Ø±Ø§ÛŒ Vmess/SSR) Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\n\n"
    md_content += "**Ù†Ú©ØªÙ‡:** Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ù‡ Ø´Ø¯Øª URL-Encode Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯ (Ø­Ø§ÙˆÛŒ ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ§Ø¯ÛŒ `%25`ØŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ ÛŒØ§ Ø¯Ø§Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø®Ø§Øµ) Ø§Ø² Ù†ØªØ§ÛŒØ¬ Ø­Ø°Ù Ø´Ø¯Ù‡â€ŒØ§Ù†Ø¯.\n\n"

    github_repository = os.environ.get('GITHUB_REPOSITORY')
    github_ref_name = os.environ.get('GITHUB_REF_NAME')

    base_url_for_link = ""
    if github_repository and github_ref_name:
        base_url_for_link = f"https://raw.githubusercontent.com/{github_repository}/{github_ref_name}"

    md_content += "## ğŸ“ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§\n\n"
    if protocol_counts:
        md_content += "| Ù¾Ø±ÙˆØªÚ©Ù„ | ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ | Ù„ÛŒÙ†Ú© |\n"
        md_content += "|---|---|---|\n"
        for category, count in sorted(protocol_counts.items()):
            file_name_display = f"{category}.txt"
            file_path_in_repo = f"{OUTPUT_DIR}/{category}.txt"
            link_url = f"./{file_path_in_repo}" # Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù„ÛŒÙ†Ú© Ù†Ø³Ø¨ÛŒ
            if base_url_for_link:
                link_url = f"{base_url_for_link}/{file_path_in_repo}"
            md_content += f"| {category} | {count} | [`{file_name_display}`]({link_url}) |\n"
    else:
        md_content += "Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù¾Ø±ÙˆØªÚ©Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.\n"
    md_content += "\n"

    md_content += "## ğŸŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ú©Ø´ÙˆØ±Ù‡Ø§ (Ø­Ø§ÙˆÛŒ Ú©Ø§Ù†ÙÛŒÚ¯)\n\n"
    if country_counts:
        md_content += "| Ù¾Ø±Ú†Ù… | Ú©Ø´ÙˆØ± | ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø±ØªØ¨Ø· | Ù„ÛŒÙ†Ú© |\n"
        md_content += "|:---:|---|---|---|\n"
        for category, count in sorted(country_counts.items()):
            flag_emoji = country_flags_map.get(category, DEFAULT_FLAG)
            file_name_display = f"{category}.txt"
            file_path_in_repo = f"{OUTPUT_DIR}/{category}.txt"
            link_url = f"./{file_path_in_repo}" # Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù„ÛŒÙ†Ú© Ù†Ø³Ø¨ÛŒ
            if base_url_for_link:
                link_url = f"{base_url_for_link}/{file_path_in_repo}"
            md_content += f"| {flag_emoji} | {category} | {count} | [`{file_name_display}`]({link_url}) |\n"
    else:
        md_content += "Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ú©Ø´ÙˆØ±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.\n"
    md_content += "\n"

    try:
        with open(README_FILE, 'w', encoding='utf-8') as f:
            f.write(md_content)
        logging.info(f"Successfully generated {README_FILE}")
    except Exception as e:
        logging.error(f"Failed to write {README_FILE}: {e}")


async def main():
    overall_start_time = time.time() # Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯ Ø²Ù…Ø§Ù† Ú©Ù„
    logging.info("--- Script Started ---")

    if not os.path.exists(URLS_FILE) or not os.path.exists(KEYWORDS_FILE):
        logging.critical(f"Input files not found. Ensure {URLS_FILE} and {KEYWORDS_FILE} exist.")
        return

    with open(URLS_FILE, 'r', encoding='utf-8') as f: # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† encoding='utf-8'
        urls = [line.strip() for line in f if line.strip() and not line.startswith('#')] # Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† Ø®Ø·ÙˆØ· Ú©Ø§Ù…Ù†Øª Ø´Ø¯Ù‡
    with open(KEYWORDS_FILE, 'r', encoding='utf-8') as f:
        all_categories_data = json.load(f)

    patterns_for_protocols = {}
    country_keywords_map = {}
    country_flags_from_keywords = {}

    for category_name, keywords_or_patterns_list in all_categories_data.items():
        if category_name in PROTOCOL_CATEGORIES:
            patterns_for_protocols[category_name] = keywords_or_patterns_list
        else:
            country_keywords_map[category_name] = []
            if keywords_or_patterns_list:
                potential_flag = keywords_or_patterns_list[-1]
                # ÛŒÚ© Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø¢Ø®Ø±ÛŒÙ† Ø¢ÛŒØªÙ… Ø´Ø¨ÛŒÙ‡ Ù¾Ø±Ú†Ù… Ø§Ø³Øª
                is_likely_flag = (len(potential_flag) == 2 and all(0x1F1E6 <= ord(char) <= 0x1F1FF for char in potential_flag)) or \
                                 (len(potential_flag) == 1 and ord(potential_flag) > 255 and potential_flag not in ['ä¸­å›½', 'æ—¥æœ¬', 'éŸ“å›½']) # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† ÛŒÚ© Ø§Ø³ØªØ«Ù†Ø§ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ú†Ù…â€ŒÙ‡Ø§ÛŒ ØªÚ© Ú©Ø§Ø±Ø§Ú©ØªØ±ÛŒ ØºÛŒØ±Ù…Ø¹Ù…ÙˆÙ„ Ø¯Ø± Ø§ÛŒÙ† Ø²Ù…ÛŒÙ†Ù‡

                if is_likely_flag:
                    country_flags_from_keywords[category_name] = potential_flag
                    if len(keywords_or_patterns_list) > 1:
                        country_keywords_map[category_name] = keywords_or_patterns_list[:-1]
                    else: # Ø§Ú¯Ø± ÙÙ‚Ø· Ù¾Ø±Ú†Ù… Ø¨ÙˆØ¯ØŒ Ù†Ø§Ù… Ø¯Ø³ØªÙ‡ Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                        country_keywords_map[category_name] = [category_name]
                else:
                    country_keywords_map[category_name] = keywords_or_patterns_list
            else:
                 country_keywords_map[category_name] = [category_name]


    country_category_names = list(country_keywords_map.keys())

    logging.info(f"Loaded {len(urls)} URLs and {len(all_categories_data)} categories.")
    # logging.debug(f"Protocol categories patterns: {patterns_for_protocols}")
    # logging.debug(f"Country keywords map: {country_keywords_map}")
    # logging.debug(f"Extracted country flags: {country_flags_from_keywords}")


    logging.info("Starting URL fetching...")
    fetch_start_time = time.time()
    sem = asyncio.Semaphore(CONCURRENT_REQUESTS)
    async def fetch_with_sem(session, url_item): # ØªØºÛŒÛŒØ± Ù†Ø§Ù… Ù¾Ø§Ø±Ø§Ù…ØªØ± Ø¨Ø±Ø§ÛŒ ÙˆØ¶ÙˆØ­
        async with sem:
            return await fetch_url(session, url_item) # Ø§Ø±Ø³Ø§Ù„ url_item
    async with aiohttp.ClientSession() as session:
        fetched_pages = await asyncio.gather(*[fetch_with_sem(session, u) for u in urls]) # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² u
    logging.info(f"URL fetching finished in {time.time() - fetch_start_time:.2f} seconds. Fetched {len([p for p in fetched_pages if p[1] is not None])}/{len(urls)} pages successfully.")


    final_configs_by_country = {cat: set() for cat in country_category_names}
    final_all_protocols = {cat: set() for cat in PROTOCOL_CATEGORIES}


    logging.info("Processing pages for config extraction and name association...")
    process_start_time = time.time()
    unique_configs_overall = set() # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù¾Ø±Ø¯Ø§Ø²Ø´ Ú†Ù†Ø¯Ø¨Ø§Ø±Ù‡ ÛŒÚ© Ú©Ø§Ù†ÙÛŒÚ¯ Ø§Ú¯Ø± Ø¯Ø± ØµÙØ­Ø§Øª Ù…Ø®ØªÙ„Ù ØªÚ©Ø±Ø§Ø± Ø´ÙˆØ¯

    for url_source, text_content in fetched_pages: # ØªØºÛŒÛŒØ± Ù†Ø§Ù… Ù…ØªØºÛŒØ±Ù‡Ø§ Ø¨Ø±Ø§ÛŒ ÙˆØ¶ÙˆØ­
        if not text_content:
            continue

        page_protocol_matches = find_matches(text_content, patterns_for_protocols)

        current_page_configs_unfiltered = set()
        for protocol_cat, configs_found in page_protocol_matches.items():
            for config in configs_found:
                current_page_configs_unfiltered.add(config) # Ù‡Ù…Ù‡ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡ Ø¯Ø± ØµÙØ­Ù‡ØŒ Ù‚Ø¨Ù„ Ø§Ø² ÙÛŒÙ„ØªØ± Ú©Ù„ÛŒ

        # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ùˆ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù„ÛŒØ³Øªâ€ŒÙ‡Ø§ÛŒ Ù†Ù‡Ø§ÛŒÛŒ Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§
        for config in current_page_configs_unfiltered:
            if config in unique_configs_overall: # Ø§Ú¯Ø± Ø§ÛŒÙ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù‚Ø¨Ù„Ø§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ØŒ Ø§Ø² Ø¢Ù† Ø¨Ú¯Ø°Ø±
                continue
            
            # <--- ÙØ±Ø§Ø®ÙˆØ§Ù†ÛŒ should_filter_config Ø¨Ø§ url_source --- >
            if should_filter_config(config, url_source):
                unique_configs_overall.add(config) # Ø§Ø¶Ø§ÙÙ‡ Ø¨Ù‡ Ø¯ÛŒØ¯Ù‡ Ø´Ø¯Ù‡â€ŒÙ‡Ø§ ØªØ§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ Ù„Ø§Ú¯ Ù†Ø´ÙˆØ¯ Ø§Ú¯Ø± ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯
                continue
            
            unique_configs_overall.add(config) # Ø§Ø¶Ø§ÙÙ‡ Ø¨Ù‡ Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ú©Ù„ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØ¯Ù‡ Ø´Ø¯Ù‡ (ÙÛŒÙ„ØªØ± Ù†Ø´Ø¯Ù‡)
            
            # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ final_all_protocols Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ Ù¾Ø±ÙˆØªÚ©Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´Ø¯Ù‡ Ø§Ø² find_matches
            # Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø§Ø± Ø¨Ø§ÛŒØ¯ Ø¨Ø¯Ø§Ù†ÛŒÙ… Ø§ÛŒÙ† config Ø®Ø§Øµ Ø§Ø² Ú©Ø¯Ø§Ù… protocol_cat Ø¯Ø± find_matches Ø¢Ù…Ø¯Ù‡ Ø§Ø³Øª.
            # Ø±Ø§Ù‡ Ø³Ø§Ø¯Ù‡â€ŒØªØ±: Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ±ØŒ Ù†ÙˆØ¹ Ù¾Ø±ÙˆØªÚ©Ù„ Ø±Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªØ´Ø®ÛŒØµ Ø¯Ù‡ÛŒÙ… ÛŒØ§ find_matches Ø±Ø§ Ø·ÙˆØ±ÛŒ ØªØºÛŒÛŒØ± Ø¯Ù‡ÛŒÙ… Ú©Ù‡ Ù†ÙˆØ¹ Ø±Ø§ Ù‡Ù… Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯.
            # Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø¯Ú¯ÛŒ ÙØ¹Ù„ÛŒØŒ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø§Ú¯Ø± Ú©Ø§Ù†ÙÛŒÚ¯ÛŒ Ø§Ø² find_matches Ø¢Ù…Ø¯Ù‡ØŒ Ù†ÙˆØ¹ Ø¢Ù† Ù…Ø´Ø®Øµ Ø§Ø³Øª.
            # Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø§Ø²Ù†Ú¯Ø±ÛŒ Ø¯Ø§Ø±Ø¯ Ø§Ú¯Ø± ÛŒÚ© Ú©Ø§Ù†ÙÛŒÚ¯ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ ØªÙˆØ³Ø· Ú†Ù†Ø¯ÛŒÙ† Ù¾ØªØ±Ù† Ù¾Ø±ÙˆØªÚ©Ù„ Ù…Ú† Ø´ÙˆØ¯.
            # ÙØ±Ø¶ ÙØ¹Ù„ÛŒ: Ù‡Ø± Ú©Ø§Ù†ÙÛŒÚ¯ Ù…ØªØ¹Ù„Ù‚ Ø¨Ù‡ ÛŒÚ© Ù¾Ø±ÙˆØªÚ©Ù„ Ø§Ø³Øª Ú©Ù‡ find_matches Ø¢Ù† Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù‡.

            # Ø¨Ø±Ø§ÛŒ Ø§ÙØ²ÙˆØ¯Ù† ØµØ­ÛŒØ­ Ø¨Ù‡ final_all_protocolsØŒ Ø¨Ø§ÛŒØ¯ Ø¨Ø¯Ø§Ù†ÛŒÙ… Ø§ÛŒÙ† config Ø§Ø² Ú©Ø¯Ø§Ù… Ø¯Ø³ØªÙ‡ Ù¾Ø±ÙˆØªÚ©Ù„ Ø§Ø³Øª.
            # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø§ÛŒÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ø§Ø² page_protocol_matches Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ú©Ù†ÛŒÙ….
            config_protocol_origin = None
            for p_cat, p_configs in page_protocol_matches.items():
                if config in p_configs:
                    config_protocol_origin = p_cat
                    break
            
            if config_protocol_origin and config_protocol_origin in final_all_protocols:
                 final_all_protocols[config_protocol_origin].add(config)


            # ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø§ Ú©Ø´ÙˆØ±Ù‡Ø§
            name_to_check = None
            if '#' in config:
                try:
                    potential_name = config.split('#', 1)[1]
                    name_to_check = unquote(potential_name).strip()
                    if not name_to_check: name_to_check = None
                except IndexError: pass

            if not name_to_check:
                if config.startswith('ssr://'): name_to_check = get_ssr_name(config)
                elif config.startswith('vmess://'): name_to_check = get_vmess_name(config)
                # Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø¨Ø±Ø§ÛŒ Ø³Ø§ÛŒØ± Ù¾Ø±ÙˆØªÚ©Ù„â€ŒÙ‡Ø§ Ù†ÛŒØ² ØªØ§Ø¨Ø¹ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†Ø§Ù… Ù…Ø´Ø§Ø¨Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯

            if not name_to_check:
                # Ø§Ú¯Ø± Ù†Ø§Ù…ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´ÙˆØ± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ØŒ Ø§ÛŒÙ† Ú©Ø§Ù†ÙÛŒÚ¯ Ø¨Ù‡ Ù‡ÛŒÚ† Ú©Ø´ÙˆØ±ÛŒ Ù…Ø±ØªØ¨Ø· Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯
                # Ø§Ù…Ø§ Ù‡Ù…Ú†Ù†Ø§Ù† Ø¯Ø± Ù„ÛŒØ³Øª Ù¾Ø±ÙˆØªÚ©Ù„ Ù…Ø±Ø¨ÙˆØ·Ù‡â€ŒØ§Ø´ (Ø§Ú¯Ø± Ù†ÙˆØ¹Ø´ Ù…Ø´Ø®Øµ Ø¨Ø§Ø´Ø¯) Ø¨Ø§Ù‚ÛŒ Ù…ÛŒâ€ŒÙ…Ø§Ù†Ø¯
                continue


            # Ø¨Ø±Ø±Ø³ÛŒ Ú©Ø´ÙˆØ± Ø¨Ø§ Ù†Ø§Ù… Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡
            country_match_for_this_config = False
            for country_name, keywords_for_country in country_keywords_map.items():
                for keyword in keywords_for_country:
                    if not keyword.strip(): continue # Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ† Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø®Ø§Ù„ÛŒ

                    match_found_for_keyword = False
                    # Ø¨Ø±Ø±Ø³ÛŒ Ù…Ø®ÙÙâ€ŒÙ‡Ø§ Ø¨Ø§ Ù…Ø±Ø² Ú©Ù„Ù…Ù‡
                    # Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ú©Ù‡ Û² ÛŒØ§ Û³ Ø­Ø±ÙÛŒ Ùˆ Ù‡Ù…Ú¯ÛŒ Ø¨Ø²Ø±Ú¯ Ù‡Ø³ØªÙ†Ø¯ Ø±Ø§ Ù…Ø®ÙÙ Ø¯Ø± Ù†Ø¸Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±ÛŒÙ…
                    is_abbr = (len(keyword) == 2 or len(keyword) == 3) and keyword.isupper()

                    try:
                        if is_abbr:
                            # Ø¨Ø±Ø§ÛŒ Ù…Ø®ÙÙâ€ŒÙ‡Ø§ØŒ Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø§ Ù…Ø±Ø² Ú©Ù„Ù…Ù‡ (\b) Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯
                            pattern_abbr = r'\b' + re.escape(keyword) + r'\b'
                            if re.search(pattern_abbr, name_to_check, re.IGNORECASE):
                                match_found_for_keyword = True
                        else:
                            # Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ØŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø§Ø¯Ù‡â€ŒØªØ± (Ø¨Ø¯ÙˆÙ† Ù…Ø±Ø² Ú©Ù„Ù…Ù‡ Ø§Ø¬Ø¨Ø§Ø±ÛŒ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§)
                            # Ø§Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…Ø±Ø² Ú©Ù„Ù…Ù‡ Ø±Ø§ Ù‡Ù… Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ± Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯
                            pattern_keyword = r'(?i)\b' + re.escape(keyword) + r'\b' # Ø¬Ø³ØªØ¬ÙˆÛŒ case-insensitive Ø¨Ø§ Ù…Ø±Ø² Ú©Ù„Ù…Ù‡
                            if re.search(pattern_keyword, name_to_check):
                                match_found_for_keyword = True
                            elif keyword.lower() in name_to_check.lower(): # Ø­Ø§Ù„Øª Ù¾Ø´ØªÛŒØ¨Ø§Ù† Ø¨Ø¯ÙˆÙ† Ù…Ø±Ø² Ú©Ù„Ù…Ù‡
                                match_found_for_keyword = True


                    except re.error as e:
                        logging.error(f"Regex error during country keyword matching for keyword '{keyword}' in name '{name_to_check}': {e}")
                        continue # Ø¨Ø±Ùˆ Ø¨Ù‡ Ú©Ù„Ù…Ù‡ Ú©Ù„ÛŒØ¯ÛŒ Ø¨Ø¹Ø¯ÛŒ

                    if match_found_for_keyword:
                        final_configs_by_country[country_name].add(config)
                        country_match_for_this_config = True # Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ú©Ø§Ù†ÙÛŒÚ¯ ÛŒÚ© Ú©Ø´ÙˆØ± Ù¾ÛŒØ¯Ø§ Ø´Ø¯
                        break # Ø´Ú©Ø³ØªÙ† Ø­Ù„Ù‚Ù‡ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ Ø§ÛŒÙ† Ú©Ø´ÙˆØ±
                
                if country_match_for_this_config:
                    break # Ø´Ú©Ø³ØªÙ† Ø­Ù„Ù‚Ù‡ Ú©Ø´ÙˆØ±Ù‡Ø§ØŒ Ú†ÙˆÙ† Ø§ÙˆÙ„ÛŒÙ† ØªØ·Ø§Ø¨Ù‚ Ú©Ø§ÙÛŒ Ø§Ø³Øª

    logging.info(f"Processing and aggregation finished in {time.time() - process_start_time:.2f} seconds.")
    logging.info(f"Total unique configs found before country association: {len(unique_configs_overall)}")


    # --- Save Output Files ---
    if os.path.exists(OUTPUT_DIR):
        shutil.rmtree(OUTPUT_DIR)
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    logging.info(f"Saving files to directory: {OUTPUT_DIR}")

    protocol_counts = {}
    country_counts = {}

    for category, items in final_all_protocols.items():
        if items:
            saved, count = save_to_file(OUTPUT_DIR, category, items)
            if saved: protocol_counts[category] = count

    for category, items in final_configs_by_country.items():
        if items:
            saved, count = save_to_file(OUTPUT_DIR, category, items)
            if saved: country_counts[category] = count

    logging.info("Generating README...")
    readme_start_time = time.time()
    generate_simple_readme(protocol_counts, country_counts, country_flags_from_keywords)
    logging.info(f"README generation finished in {time.time() - readme_start_time:.2f} seconds.")

    logging.info(f"--- Script Finished in {time.time() - overall_start_time:.2f} seconds ---")

if __name__ == "__main__":
    asyncio.run(main())
