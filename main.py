# Ù…Ø³ÛŒØ±: main.py

import asyncio
import aiohttp
from config import settings
from utils.file_handler import read_urls_from_file, load_keywords
from utils.logger_setup import setup_logger
from core.fetcher import fetch_and_normalize_content
from core.parser import analyze_content # Ø§ÛŒÙ…Ù¾ÙˆØ±Øª ØªØ§Ø¨Ø¹ ØªØ­Ù„ÛŒÙ„Ú¯Ø±

async def main():
    """
    Ù†Ù‚Ø·Ù‡ Ø´Ø±ÙˆØ¹ Ø§ØµÙ„ÛŒ Ø¨Ø±Ù†Ø§Ù…Ù‡
    """
    logger = setup_logger()
    logger.info("="*50)
    logger.info("Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø§Ù†ÙÛŒÚ¯â€ŒÙ‡Ø§ Ø´Ø±ÙˆØ¹ Ø¨Ù‡ Ú©Ø§Ø± Ú©Ø±Ø¯")
    logger.info("="*50)

    # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ (Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ Ùˆ Ú©Ù„Ù…Ø§Øª Ú©Ù„ÛŒØ¯ÛŒ)
    plain_urls = read_urls_from_file(settings.PLAIN_CONTENT_URLS_FILE)
    base64_urls = read_urls_from_file(settings.BASE64_CONTENT_URLS_FILE)
    keywords = load_keywords(settings.KEYWORDS_FILE)

    if not plain_urls and not base64_urls:
        logger.error("Ù‡ÛŒÚ† URL Ø§ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        return
    if not keywords:
        logger.error("ÙØ§ÛŒÙ„ keywords.json ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª. Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        return

    logger.info(f"Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´Ø¯: {len(plain_urls)} Ù„ÛŒÙ†Ú© Ø¹Ø§Ø¯ÛŒØŒ {len(base64_urls)} Ù„ÛŒÙ†Ú© Base64ØŒ Ùˆ {len(keywords)} Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡.")

    tasks = []
    async with aiohttp.ClientSession() as session:
        for url in plain_urls:
            tasks.append(fetch_and_normalize_content(session, url, is_base64_content=False, logger=logger))
        
        for url in base64_urls:
            tasks.append(fetch_and_normalize_content(session, url, is_base64_content=True, logger=logger))

        results = await asyncio.gather(*tasks)

    logger.info("--- Ø´Ø±ÙˆØ¹ ØªØ­Ù„ÛŒÙ„ Ùˆ Ø¢Ù…Ø§Ø±Ø¯Ù‡ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø´Ø¯Ù‡ ---")
    
    all_normalized_content = ""
    successful_fetches = 0
    for url, content in results:
        if content:
            successful_fetches += 1
            # ØªØ­Ù„ÛŒÙ„ Ù…Ø­ØªÙˆØ§ÛŒ Ù‡Ø± Ù„ÛŒÙ†Ú© Ø¨Ù‡ ØµÙˆØ±Øª Ø¬Ø¯Ø§Ú¯Ø§Ù†Ù‡
            stats = analyze_content(content, keywords)
            if stats['total'] > 0:
                # ÙØ±Ù…Øªâ€ŒØ¯Ù‡ÛŒ Ø²ÛŒØ¨Ø§ Ø¨Ø±Ø§ÛŒ Ù„Ø§Ú¯ Ø¢Ù…Ø§Ø±
                protocol_stats_str = ", ".join([f"{p}: {c}" for p, c in stats['protocols'].items()])
                logger.info(
                    f"ğŸ“Š Ø¢Ù…Ø§Ø± Ø¨Ø±Ø§ÛŒ {url} -> "
                    f"Ú©Ù„: {stats['total']}, "
                    f"Ø§ÛŒØ±Ø§Ù†: {stats['iran_count']}, "
                    f"[{protocol_stats_str}]"
                )
            else:
                logger.info(f"âšªï¸ Ø¨Ø±Ø§ÛŒ {url} Ù‡ÛŒÚ† Ú©Ø§Ù†ÙÛŒÚ¯ Ù‚Ø§Ø¨Ù„ ØªØ´Ø®ÛŒØµÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            
            all_normalized_content += content + "\n"
    
    logger.info(f"ØªØ­Ù„ÛŒÙ„ Ø¨Ø±Ø§ÛŒ {successful_fetches} Ù„ÛŒÙ†Ú© Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
    
    logger.info("="*50)
    logger.info("Ø§Ø¬Ø±Ø§ÛŒ Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ù‡ Ù¾Ø§ÛŒØ§Ù† Ø±Ø³ÛŒØ¯")
    logger.info("="*50)


if __name__ == "__main__":
    asyncio.run(main())
